{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtnQo0CoGCRRnkKnkB5Uve",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariamHawwari/MariamHawwari/blob/main/TF_IDF_Mariam_Hawwari_(Midterm_Project).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Midterm Project:** Arabic Text Tokenization, Stem/Lem & TF-IDF Analysis"
      ],
      "metadata": {
        "id": "n6XRqOYAmr6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Student:** Mariam Hawwari <br>\n",
        "*   **Course:** Indexation <br>\n",
        "*   **Instructor:** Dr. Alaf Makke <br>\n",
        "*   **Objective:** Preprocess Arabic text by removing stopwords, applying stem or lem, and calculating TF-IDF scores for a set of sentences.<br>\n",
        "*   **Steps:** 08 steps\n",
        "<br>"
      ],
      "metadata": {
        "id": "Y7B2JKndkNCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **01. Install & Import Libraries**"
      ],
      "metadata": {
        "id": "C6Z8IvpYBnOI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTPRAdbD_jZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c291f73-83dc-4cdf-bce0-9a7fc7555b44",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: qalsadi in /usr/local/lib/python3.11/dist-packages (0.5)\n",
            "Requirement already satisfied: Arabic-Stopwords>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from qalsadi) (0.4.3)\n",
            "Requirement already satisfied: alyahmor>=0.2 in /usr/local/lib/python3.11/dist-packages (from qalsadi) (0.2)\n",
            "Requirement already satisfied: arramooz-pysqlite>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from qalsadi) (0.4.2)\n",
            "Requirement already satisfied: codernitydb3 in /usr/local/lib/python3.11/dist-packages (from qalsadi) (0.6.0)\n",
            "Requirement already satisfied: libqutrub>=1.2.3 in /usr/local/lib/python3.11/dist-packages (from qalsadi) (1.2.4.1)\n",
            "Requirement already satisfied: mysam-tagmanager>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from qalsadi) (0.4)\n",
            "Requirement already satisfied: naftawayh>=0.3 in /usr/local/lib/python3.11/dist-packages (from qalsadi) (0.4)\n",
            "Requirement already satisfied: pickledb>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from qalsadi) (1.3.2)\n",
            "Requirement already satisfied: pyarabic>=0.6.7 in /usr/local/lib/python3.11/dist-packages (from qalsadi) (0.6.15)\n",
            "Requirement already satisfied: tashaphyne>=0.3.4.1 in /usr/local/lib/python3.11/dist-packages (from qalsadi) (0.3.6)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from pickledb>=0.9.2->qalsadi) (3.10.15)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.11/dist-packages (from pickledb>=0.9.2->qalsadi) (24.1.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from pyarabic>=0.6.7->qalsadi) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install qalsadi\n",
        "\n",
        "import nltk  # Natural Language Toolkit: A library for natural language processing.\n",
        "nltk.download('stopwords')  # Downloads a list of common stopwords for various languages.\n",
        "nltk.download('punkt_tab') ## Download the Punkt tokenizer model for sentence tokenization.\n",
        "nltk.download('punkt')  # Downloads a tokenizer model that splits text into sentences and words.\n",
        "import pandas as pd  # Import pandas to create a DataFrame.\n",
        "import string # Import string module for punctuation\n",
        "\n",
        "from nltk.corpus import stopwords  # Provides access to the stopwords for filtering common words.\n",
        "from nltk.tokenize import word_tokenize  # Tokenizer to split text into words.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Import TfidfVectorizer for TF-IDF calculation.\n",
        "from qalsadi.lemmatizer import Lemmatizer  # Arabic lemmatizer from qalsadi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **02. Define Arabic Sentences**"
      ],
      "metadata": {
        "id": "tB9wo0ZemzwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = \"أنا أذهب إلى المدرسة كل يوم.\"\n",
        "sentence2 = \"المدرسة مكان رائع للتعلم.\"\n",
        "sentence3 = \"التعلم يساعدنا في تحقيق النجاح.\""
      ],
      "metadata": {
        "id": "JMudTCe_qdaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **03. Load Arabic Stopwords**"
      ],
      "metadata": {
        "id": "AFHA8JcEnKUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load from an external library (NLTK)\n",
        "arabic_stopwords = stopwords.words('arabic')\n",
        "print(arabic_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYbfottMEKFC",
        "outputId": "29d7bf41-fc6b-4b6a-f907-c6173b03dc4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', 'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', 'اللواتي', 'إلى', 'إليك', 'إليكم', 'إليكما', 'إليكن', 'أم', 'أما', 'أما', 'إما', 'أن', 'إن', 'إنا', 'أنا', 'أنت', 'أنتم', 'أنتما', 'أنتن', 'إنما', 'إنه', 'أنى', 'أنى', 'آه', 'آها', 'أو', 'أولاء', 'أولئك', 'أوه', 'آي', 'أي', 'أيها', 'إي', 'أين', 'أين', 'أينما', 'إيه', 'بخ', 'بس', 'بعد', 'بعض', 'بك', 'بكم', 'بكم', 'بكما', 'بكن', 'بل', 'بلى', 'بما', 'بماذا', 'بمن', 'بنا', 'به', 'بها', 'بهم', 'بهما', 'بهن', 'بي', 'بين', 'بيد', 'تلك', 'تلكم', 'تلكما', 'ته', 'تي', 'تين', 'تينك', 'ثم', 'ثمة', 'حاشا', 'حبذا', 'حتى', 'حيث', 'حيثما', 'حين', 'خلا', 'دون', 'ذا', 'ذات', 'ذاك', 'ذان', 'ذانك', 'ذلك', 'ذلكم', 'ذلكما', 'ذلكن', 'ذه', 'ذو', 'ذوا', 'ذواتا', 'ذواتي', 'ذي', 'ذين', 'ذينك', 'ريث', 'سوف', 'سوى', 'شتان', 'عدا', 'عسى', 'عل', 'على', 'عليك', 'عليه', 'عما', 'عن', 'عند', 'غير', 'فإذا', 'فإن', 'فلا', 'فمن', 'في', 'فيم', 'فيما', 'فيه', 'فيها', 'قد', 'كأن', 'كأنما', 'كأي', 'كأين', 'كذا', 'كذلك', 'كل', 'كلا', 'كلاهما', 'كلتا', 'كلما', 'كليكما', 'كليهما', 'كم', 'كم', 'كما', 'كي', 'كيت', 'كيف', 'كيفما', 'لا', 'لاسيما', 'لدى', 'لست', 'لستم', 'لستما', 'لستن', 'لسن', 'لسنا', 'لعل', 'لك', 'لكم', 'لكما', 'لكن', 'لكنما', 'لكي', 'لكيلا', 'لم', 'لما', 'لن', 'لنا', 'له', 'لها', 'لهم', 'لهما', 'لهن', 'لو', 'لولا', 'لوما', 'لي', 'لئن', 'ليت', 'ليس', 'ليسا', 'ليست', 'ليستا', 'ليسوا', 'ما', 'ماذا', 'متى', 'مذ', 'مع', 'مما', 'ممن', 'من', 'منه', 'منها', 'منذ', 'مه', 'مهما', 'نحن', 'نحو', 'نعم', 'ها', 'هاتان', 'هاته', 'هاتي', 'هاتين', 'هاك', 'هاهنا', 'هذا', 'هذان', 'هذه', 'هذي', 'هذين', 'هكذا', 'هل', 'هلا', 'هم', 'هما', 'هن', 'هنا', 'هناك', 'هنالك', 'هو', 'هؤلاء', 'هي', 'هيا', 'هيت', 'هيهات', 'والذي', 'والذين', 'وإذ', 'وإذا', 'وإن', 'ولا', 'ولكن', 'ولو', 'وما', 'ومن', 'وهو', 'يا', 'أبٌ', 'أخٌ', 'حمٌ', 'فو', 'أنتِ', 'يناير', 'فبراير', 'مارس', 'أبريل', 'مايو', 'يونيو', 'يوليو', 'أغسطس', 'سبتمبر', 'أكتوبر', 'نوفمبر', 'ديسمبر', 'جانفي', 'فيفري', 'مارس', 'أفريل', 'ماي', 'جوان', 'جويلية', 'أوت', 'كانون', 'شباط', 'آذار', 'نيسان', 'أيار', 'حزيران', 'تموز', 'آب', 'أيلول', 'تشرين', 'دولار', 'دينار', 'ريال', 'درهم', 'ليرة', 'جنيه', 'قرش', 'مليم', 'فلس', 'هللة', 'سنتيم', 'يورو', 'ين', 'يوان', 'شيكل', 'واحد', 'اثنان', 'ثلاثة', 'أربعة', 'خمسة', 'ستة', 'سبعة', 'ثمانية', 'تسعة', 'عشرة', 'أحد', 'اثنا', 'اثني', 'إحدى', 'ثلاث', 'أربع', 'خمس', 'ست', 'سبع', 'ثماني', 'تسع', 'عشر', 'ثمان', 'سبت', 'أحد', 'اثنين', 'ثلاثاء', 'أربعاء', 'خميس', 'جمعة', 'أول', 'ثان', 'ثاني', 'ثالث', 'رابع', 'خامس', 'سادس', 'سابع', 'ثامن', 'تاسع', 'عاشر', 'حادي', 'أ', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ء', 'ى', 'آ', 'ؤ', 'ئ', 'أ', 'ة', 'ألف', 'باء', 'تاء', 'ثاء', 'جيم', 'حاء', 'خاء', 'دال', 'ذال', 'راء', 'زاي', 'سين', 'شين', 'صاد', 'ضاد', 'طاء', 'ظاء', 'عين', 'غين', 'فاء', 'قاف', 'كاف', 'لام', 'ميم', 'نون', 'هاء', 'واو', 'ياء', 'همزة', 'ي', 'نا', 'ك', 'كن', 'ه', 'إياه', 'إياها', 'إياهما', 'إياهم', 'إياهن', 'إياك', 'إياكما', 'إياكم', 'إياك', 'إياكن', 'إياي', 'إيانا', 'أولالك', 'تانِ', 'تانِك', 'تِه', 'تِي', 'تَيْنِ', 'ثمّ', 'ثمّة', 'ذانِ', 'ذِه', 'ذِي', 'ذَيْنِ', 'هَؤلاء', 'هَاتانِ', 'هَاتِه', 'هَاتِي', 'هَاتَيْنِ', 'هَذا', 'هَذانِ', 'هَذِه', 'هَذِي', 'هَذَيْنِ', 'الألى', 'الألاء', 'أل', 'أنّى', 'أيّ', 'ّأيّان', 'أنّى', 'أيّ', 'ّأيّان', 'ذيت', 'كأيّ', 'كأيّن', 'بضع', 'فلان', 'وا', 'آمينَ', 'آهِ', 'آهٍ', 'آهاً', 'أُفٍّ', 'أُفٍّ', 'أفٍّ', 'أمامك', 'أمامكَ', 'أوّهْ', 'إلَيْكَ', 'إلَيْكَ', 'إليكَ', 'إليكنّ', 'إيهٍ', 'بخٍ', 'بسّ', 'بَسْ', 'بطآن', 'بَلْهَ', 'حاي', 'حَذارِ', 'حيَّ', 'حيَّ', 'دونك', 'رويدك', 'سرعان', 'شتانَ', 'شَتَّانَ', 'صهْ', 'صهٍ', 'طاق', 'طَق', 'عَدَسْ', 'كِخ', 'مكانَك', 'مكانَك', 'مكانَك', 'مكانكم', 'مكانكما', 'مكانكنّ', 'نَخْ', 'هاكَ', 'هَجْ', 'هلم', 'هيّا', 'هَيْهات', 'وا', 'واهاً', 'وراءَك', 'وُشْكَانَ', 'وَيْ', 'يفعلان', 'تفعلان', 'يفعلون', 'تفعلون', 'تفعلين', 'اتخذ', 'ألفى', 'تخذ', 'ترك', 'تعلَّم', 'جعل', 'حجا', 'حبيب', 'خال', 'حسب', 'خال', 'درى', 'رأى', 'زعم', 'صبر', 'ظنَّ', 'عدَّ', 'علم', 'غادر', 'ذهب', 'وجد', 'ورد', 'وهب', 'أسكن', 'أطعم', 'أعطى', 'رزق', 'زود', 'سقى', 'كسا', 'أخبر', 'أرى', 'أعلم', 'أنبأ', 'حدَث', 'خبَّر', 'نبَّا', 'أفعل به', 'ما أفعله', 'بئس', 'ساء', 'طالما', 'قلما', 'لات', 'لكنَّ', 'ءَ', 'أجل', 'إذاً', 'أمّا', 'إمّا', 'إنَّ', 'أنًّ', 'أى', 'إى', 'أيا', 'ب', 'ثمَّ', 'جلل', 'جير', 'رُبَّ', 'س', 'علًّ', 'ف', 'كأنّ', 'كلَّا', 'كى', 'ل', 'لات', 'لعلَّ', 'لكنَّ', 'لكنَّ', 'م', 'نَّ', 'هلّا', 'وا', 'أل', 'إلّا', 'ت', 'ك', 'لمّا', 'ن', 'ه', 'و', 'ا', 'ي', 'تجاه', 'تلقاء', 'جميع', 'حسب', 'سبحان', 'شبه', 'لعمر', 'مثل', 'معاذ', 'أبو', 'أخو', 'حمو', 'فو', 'مئة', 'مئتان', 'ثلاثمئة', 'أربعمئة', 'خمسمئة', 'ستمئة', 'سبعمئة', 'ثمنمئة', 'تسعمئة', 'مائة', 'ثلاثمائة', 'أربعمائة', 'خمسمائة', 'ستمائة', 'سبعمائة', 'ثمانمئة', 'تسعمائة', 'عشرون', 'ثلاثون', 'اربعون', 'خمسون', 'ستون', 'سبعون', 'ثمانون', 'تسعون', 'عشرين', 'ثلاثين', 'اربعين', 'خمسين', 'ستين', 'سبعين', 'ثمانين', 'تسعين', 'بضع', 'نيف', 'أجمع', 'جميع', 'عامة', 'عين', 'نفس', 'لا سيما', 'أصلا', 'أهلا', 'أيضا', 'بؤسا', 'بعدا', 'بغتة', 'تعسا', 'حقا', 'حمدا', 'خلافا', 'خاصة', 'دواليك', 'سحقا', 'سرا', 'سمعا', 'صبرا', 'صدقا', 'صراحة', 'طرا', 'عجبا', 'عيانا', 'غالبا', 'فرادى', 'فضلا', 'قاطبة', 'كثيرا', 'لبيك', 'معاذ', 'أبدا', 'إزاء', 'أصلا', 'الآن', 'أمد', 'أمس', 'آنفا', 'آناء', 'أنّى', 'أول', 'أيّان', 'تارة', 'ثمّ', 'ثمّة', 'حقا', 'صباح', 'مساء', 'ضحوة', 'عوض', 'غدا', 'غداة', 'قطّ', 'كلّما', 'لدن', 'لمّا', 'مرّة', 'قبل', 'خلف', 'أمام', 'فوق', 'تحت', 'يمين', 'شمال', 'ارتدّ', 'استحال', 'أصبح', 'أضحى', 'آض', 'أمسى', 'انقلب', 'بات', 'تبدّل', 'تحوّل', 'حار', 'رجع', 'راح', 'صار', 'ظلّ', 'عاد', 'غدا', 'كان', 'ما انفك', 'ما برح', 'مادام', 'مازال', 'مافتئ', 'ابتدأ', 'أخذ', 'اخلولق', 'أقبل', 'انبرى', 'أنشأ', 'أوشك', 'جعل', 'حرى', 'شرع', 'طفق', 'علق', 'قام', 'كرب', 'كاد', 'هبّ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **04. Preprocess & Tokenize Text**"
      ],
      "metadata": {
        "id": "M1nmbhrpJ8wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)) # Remove punctuation\n",
        "    tokens = word_tokenize(text) # Tokenize\n",
        "    filtered_tokens = []\n",
        "    for word in tokens:\n",
        "        if word not in arabic_stopwords:\n",
        "            filtered_tokens.append(word)\n",
        "    return filtered_tokens"
      ],
      "metadata": {
        "id": "dQArbdCtJ7fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **05. Lemmatize Text**\n",
        "\n",
        "* *I’m using lemmatization because my aim is to compare sentences/documents to see if they share similar **themes or topics** rather than just the overlap in word forms. Lemmatization ensures that all forms of a word are counted as the same term, reducing the noise created by variations. In this context, using stemming would produce non-words and group related words under a common root, which can obscure individual word meanings and decrease accuracy.*"
      ],
      "metadata": {
        "id": "x08LV2xJMQsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = Lemmatizer()\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatized_tokens = []\n",
        "    for word in tokens:\n",
        "        lemmatized_word = lemmatizer.lemmatize(word)\n",
        "        lemmatized_tokens.append(lemmatized_word)\n",
        "    return lemmatized_tokens"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zIVPO8eeKYXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **06. Process Results for Each Sentence**"
      ],
      "metadata": {
        "id": "8y1oZmvPMlpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_sentence1 = preprocess_text(sentence1)\n",
        "processed_sentence2 = preprocess_text(sentence2)\n",
        "processed_sentence3 = preprocess_text(sentence3)\n",
        "\n",
        "lemmatized_sentence1 = lemmatize_tokens(processed_sentence1)\n",
        "lemmatized_sentence2 = lemmatize_tokens(processed_sentence2)\n",
        "lemmatized_sentence3 = lemmatize_tokens(processed_sentence3)\n",
        "\n",
        "# Join the lemmatized tokens back into strings\n",
        "lemmatized_sentence1_str = \" \".join(lemmatized_sentence1)\n",
        "lemmatized_sentence2_str = \" \".join(lemmatized_sentence2)\n",
        "lemmatized_sentence3_str = \" \".join(lemmatized_sentence3)\n",
        "\n",
        "\n",
        "# Print results for each sentence\n",
        "print(\"Lemmatized Sentence 1:\", lemmatized_sentence1_str)\n",
        "print(\"Lemmatized Sentence 2:\", lemmatized_sentence2_str)\n",
        "print(\"Lemmatized Sentence 3:\", lemmatized_sentence3_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPlohoDPMqQ2",
        "outputId": "a10f12dc-11bd-417b-cf64-f3e8703d11a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Sentence 1: ذهب مدرس يوم\n",
            "Lemmatized Sentence 2: مدرس مكان رائع تعلم\n",
            "Lemmatized Sentence 3: تعلم ساعد تحقيق نجاح\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **07.Calculate & Display TF-IDF Scores**"
      ],
      "metadata": {
        "id": "FgUOICNfqENG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **First Method:** Display scores in a structured tabular format. <br>\n",
        "*   **Note:** The results are flipped in colab."
      ],
      "metadata": {
        "id": "VyplaRNrvDb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate TF-IDF scores\n",
        "corpus = [lemmatized_sentence1_str, lemmatized_sentence2_str, lemmatized_sentence3_str]\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the corpus (Expects a list of strings)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get feature names (terms)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame to display TF-IDF scores in columns\n",
        "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names, index=[\"Sentence1\", \"Sentence2\", \"Sentence3\"])\n",
        "\n",
        "# Display the DataFrame\n",
        "print(\"\\nTF-IDF Scores in Columns:\")\n",
        "print(df_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZwRGyZO6LQX",
        "outputId": "25cb24c2-2449-4e3d-c241-828337ec7d43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Scores in Columns:\n",
            "              تحقيق      تعلم       ذهب      رائع      ساعد      مدرس  \\\n",
            "Sentence1  0.000000  0.000000  0.622766  0.000000  0.000000  0.473630   \n",
            "Sentence2  0.000000  0.428046  0.000000  0.562829  0.000000  0.428046   \n",
            "Sentence3  0.528635  0.402040  0.000000  0.000000  0.528635  0.000000   \n",
            "\n",
            "               مكان      نجاح       يوم  \n",
            "Sentence1  0.000000  0.000000  0.622766  \n",
            "Sentence2  0.562829  0.000000  0.000000  \n",
            "Sentence3  0.000000  0.528635  0.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Second Method:** Display scores in a text format."
      ],
      "metadata": {
        "id": "HnOfvDeHtS4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate TF-IDF scores\n",
        "corpus = [lemmatized_sentence1_str, lemmatized_sentence2_str, lemmatized_sentence3_str]\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the corpus\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get feature names (terms)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Display TF-IDF scores\n",
        "for i, sentence in enumerate(corpus):\n",
        "    print(f\"\\nTF-IDF for Sentence {i+1} ({sentence}):\")\n",
        "    for j, feature in enumerate(feature_names):\n",
        "        print(f\"{feature}: {tfidf_matrix[i, j]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rurxbxl7jEuW",
        "outputId": "4dd3d636-2596-4912-fbbf-ffbe3bf391a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF for Sentence 1 (ذهب مدرس يوم):\n",
            "تحقيق: 0.0000\n",
            "تعلم: 0.0000\n",
            "ذهب: 0.6228\n",
            "رائع: 0.0000\n",
            "ساعد: 0.0000\n",
            "مدرس: 0.4736\n",
            "مكان: 0.0000\n",
            "نجاح: 0.0000\n",
            "يوم: 0.6228\n",
            "\n",
            "TF-IDF for Sentence 2 (مدرس مكان رائع تعلم):\n",
            "تحقيق: 0.0000\n",
            "تعلم: 0.4280\n",
            "ذهب: 0.0000\n",
            "رائع: 0.5628\n",
            "ساعد: 0.0000\n",
            "مدرس: 0.4280\n",
            "مكان: 0.5628\n",
            "نجاح: 0.0000\n",
            "يوم: 0.0000\n",
            "\n",
            "TF-IDF for Sentence 3 (تعلم ساعد تحقيق نجاح):\n",
            "تحقيق: 0.5286\n",
            "تعلم: 0.4020\n",
            "ذهب: 0.0000\n",
            "رائع: 0.0000\n",
            "ساعد: 0.5286\n",
            "مدرس: 0.0000\n",
            "مكان: 0.0000\n",
            "نجاح: 0.5286\n",
            "يوم: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function ArabicDictionary.__del__ at 0x7d768ed22ca0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/arramooz/arabicdictionary.py\", line 109, in __del__\n",
            "    self.db_connect.close()\n",
            "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 137949572218880 and this is thread id 137948871763520.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **08. Analyse TF/IDF Scores**"
      ],
      "metadata": {
        "id": "iyQtNNJ-wlc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts = {}\n",
        "for sentence in corpus:\n",
        "    for word in sentence.split():  # Split sentence by spaces to get words\n",
        "        word_counts[word] = word_counts.get(word, 0) + 1\n",
        "\n",
        "common_words = []\n",
        "unique_words = []\n",
        "\n",
        "for word, count in word_counts.items():\n",
        "    if count > 1:\n",
        "        common_words.append(word)\n",
        "    else:\n",
        "        unique_words.append(word)\n",
        "\n",
        "print(\"\\nCommon Words: Low TF-IDF words are generally less important for distinguishing the content/topic of a sentence or document from others:\\n\")\n",
        "for word in common_words:\n",
        "    if word in df_tfidf.columns:  # Check if the word is a valid column name\n",
        "        tfidf_scores = df_tfidf[word].values  # Get TF-IDF scores for the word\n",
        "        # Find the sentence with the highest TF-IDF score for this word (if it exists)\n",
        "        max_tfidf_index = tfidf_scores.argmax() if tfidf_scores.max() > 0 else -1\n",
        "\n",
        "        if max_tfidf_index != -1:  # if the word appeared at least once in the corpus\n",
        "            print(f\"- {word}: {tfidf_scores.max():.2f}\")  # Print the highest TF-IDF score for this word\n",
        "\n",
        "print(\"\\n\\nUnique Words: High TF-IDF words are more important for highlighting unique or relevant terms that help distinguish a sentence from others.\\n\")\n",
        "for word in unique_words:\n",
        "    if word in df_tfidf.columns:  # Check if the word is a valid column name\n",
        "        tfidf_scores = df_tfidf[word].values\n",
        "        max_tfidf_index = tfidf_scores.argmax() if tfidf_scores.max() > 0 else -1\n",
        "        if max_tfidf_index != -1:\n",
        "            print(f\"- {word}: {tfidf_scores.max():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8-bAvV3wlLW",
        "outputId": "e6656f32-e7dc-44a8-dae1-7ef03e4bf761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function ArabicDictionary.__del__ at 0x7d768ed22ca0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/arramooz/arabicdictionary.py\", line 109, in __del__\n",
            "    self.db_connect.close()\n",
            "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 137949572218880 and this is thread id 137948871763520.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Common Words: Low TF-IDF words are generally less important for distinguishing the content/topic of a sentence or document from others:\n",
            "\n",
            "- مدرس: 0.47\n",
            "- تعلم: 0.43\n",
            "\n",
            "\n",
            "Unique Words: High TF-IDF words are more important for highlighting unique or relevant terms that help distinguish a sentence from others.\n",
            "\n",
            "- ذهب: 0.62\n",
            "- يوم: 0.62\n",
            "- مكان: 0.56\n",
            "- رائع: 0.56\n",
            "- ساعد: 0.53\n",
            "- تحقيق: 0.53\n",
            "- نجاح: 0.53\n"
          ]
        }
      ]
    }
  ]
}